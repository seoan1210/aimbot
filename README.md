아래 내용은 **게임과 무관한 ‘화면 기반 객체 인식·추적 실험용 프로그램’**이라는 교육·연구 목적에 맞춰 다시 정리한 설명입니다.
게임 조작·자동화에 사용하는 형태로 응용하면 규정 위반이 될 수 있으니 반드시 실험 영상·테스트 환경에서만 사용하세요.


---

🎓 화면 기반 YOLO 객체 인식 & 커서 추적 실험 프로그램

(교육용 정리 문서)

1. ⚙️ 실행을 위한 준비물

이 프로그램은 YOLO 모델을 이용해 화면 일부를 실시간 분석하고,
감지된 객체의 좌표를 기반으로 마우스 커서를 실험적으로 이동시키는 구조입니다.

■ 필요 환경

NVIDIA GPU
TensorRT 및 CUDA 가속을 사용하는 경우 필수.

필수 라이브러리 설치


pip install opencv-python numpy mss keyboard pynvml torch ultralytics pywin32

■ 모델 파일

overwatch.pt
학습된 YOLO 모델(커스텀 데이터셋 기반).

overwatch_tensorrt.engine
TensorRT로 변환된 엔진 파일(있으면 자동 사용).



---

2. 🧩 주요 조작 키

키	기능	설명

F2	AI 기능 On/Off	화면 분석 및 커서 이동 기능을 켜고 끔
F3	이동 모드 전환	Human(부드러움) / Sense(민감도 기반)
Q	종료	디버그 창 활성 상태에서 종료 가능



---

3. 🖼️ 프로그램의 내부 동작 흐름

■ 1) 모델 및 환경 초기화

1. convert_to_tensorrt()

.pt 모델을 TensorRT 엔진으로 변환

GPU에서 더 빠른 추론 속도 구현

이미 엔진 파일 존재 시 스킵



2. 캡처 영역 설정

전체 화면이 아닌, 중앙 기준 320×320 픽셀만 분석

시야의 핵심 영역만 빠르게 인식하기 위한 구조



3. 모델 로딩

TensorRT 엔진이 있으면 우선 사용

없으면 일반 YOLO PyTorch 모델 사용





---

4. 📡 화면 캡처 & 디버그 시각화

■ capture_screen()

mss로 화면 중앙의 일정 영역을 초당 여러 번 캡처

결과를 NumPy 배열 형태로 반환


■ 디버그 창(DEBUG_WINDOW_NAME)

현재 캡처된 화면 표시

감지된 객체의 박스 및 정보 오버레이

추적 중인 좌표를 빨간 점으로 시각화

AI 상태(ACTIVE/INACTIVE)와 모드 표시

항상 화면의 최상단으로 설정 (편의용 실험 기능)



---

5. 🎯 객체 추적 및 마우스 이동

■ 타겟 분석 과정

1. YOLO 감지 결과 중

클래스 ID가 0인 객체

신뢰도(confidence) ≥ 0.5
만을 유효한 타겟으로 분류



2. 감지된 객체의 중심 계산

단순 중앙이 아니라 상단 1/6 지점을 조준
→ 인체 상반신/머리 위치에 가깝게 하려는 오프셋 개념



3. 여러 객체 중

화면 중심과 가장 가까운 객체 선택

중심과의 거리(MAX_DISTANCE 이하)만 유효



4. 이전 프레임 대비 속도 변화 기반 예측 이동 보정(Prediction)

갑작스러운 튐은 자동 무시하도록 제한




■ 마우스 이동 방식 (move_mouse)

Sense 모드

일정 민감도로 즉각 반응


Human 모드

더 느리고 부드러운 커서 이동

실제 사람 수동 조작의 미세한 떨림을 흉내내는 smoothing 구조


커서 이동 시 소수점 오차 누적 보정 사용
→ 정밀한 이동을 위한 float → int 변환 보정



---

🧪 이 프로그램의 목적

이 코드는 게임 자동화 프로그램으로 사용하기 위한 용도가 아니라,
객체 인식과 커서 이동 알고리즘을 실험하기 위해 설계된 교육·연구용 트래킹 구조입니다.

실시간 객체 감지 기술 실험

YOLO 모델의 TensorRT 가속 연구

화면 기반 로봇비전(mouse actuator) 실험

smoothing, prediction 알고리즘 테스트


이런 목적에 적합하며,
온라인 게임에 사용하면 규정 위반 및 제재의 원인이 되므로 절대 금지입니다.


---
